{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "data=pd.read_csv('dataset/data.csv')\n",
    "def one_zero(x):\n",
    "    if x==-1:\n",
    "        return 0\n",
    "    return x\n",
    "data['label']=data['label'].apply(one_zero)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "class neural_net():\n",
    "    def __init__(self,ni,nh,no,lr=0.01,max_steps=10):\n",
    "        self._lr=lr\n",
    "        self._max_steps=max_steps\n",
    "        self._n_hidden=nh\n",
    "        self._n_output=no\n",
    "        self._n_input=ni\n",
    "        self._layers=[]\n",
    "        self._labels=[]\n",
    "        hidden_layer = [{'weights':np.random.rand(self._n_input + 1),'name':'hidden_layer_1_unit_'+str(i+1)} for i in range(self._n_hidden)]\n",
    "        output_layer= [{'weights':np.random.rand(self._n_hidden ),'name':'output_layer_unit_'+str(i+1)} for i in range(self._n_output)]\n",
    "        self._layers.append(hidden_layer)\n",
    "        self._layers.append(output_layer)\n",
    "    \n",
    "    def preprocessing(self,X,Y):\n",
    "        #add bias to input\n",
    "        X=np.hstack((X,np.ones(X.shape[0]).reshape((X.shape[0],1))))\n",
    "        #one hot encode label\n",
    "        if Y is not None:\n",
    "            self._labels=list(set(Y.flatten()))\n",
    "            labels=dict(enumerate(self._labels))\n",
    "            labels={v: k for k, v in labels.items()}\n",
    "            print(labels)\n",
    "            new_y=np.zeros((Y.shape[0],len(labels)))\n",
    "            for i in range(Y.shape[0]):\n",
    "                new_y[i][labels[Y[i][0]]]=1\n",
    "            Y=new_y\n",
    "        return X,Y\n",
    "    \n",
    "    def activation(self,x,name='sigmoid'):\n",
    "        if name=='sigmoid':\n",
    "            return 1./(1.+np.exp(-x))\n",
    "        else:\n",
    "            return 1.\n",
    "    \n",
    "    def output(self,inputs,weights):\n",
    "        return np.dot(weights.T,inputs)\n",
    "    \n",
    "    def forward_prop(self,row):\n",
    "        inputs=row\n",
    "        for layer in self._layers:\n",
    "            new_inputs=[]\n",
    "            for unit in layer:\n",
    "                x=self.output(inputs,unit['weights'])\n",
    "                unit['output']=self.activation(x)\n",
    "                new_inputs.append(unit['output'])\n",
    "            inputs=np.array(new_inputs)\n",
    "        return inputs\n",
    "    \n",
    "    def derivate_b(self,x):\n",
    "        return x*(1-x)\n",
    "    \n",
    "    def back_prop_error(self,y):\n",
    "        for i in reversed(range(len(self._layers))):\n",
    "            layer=self._layers[i]\n",
    "            if i==len(self._layers)-1:\n",
    "                for j in range(len(layer)):\n",
    "                    layer[j]['delta']=y[j]-layer[j]['output']\n",
    "            else:   \n",
    "                for j in range(len(layer)):\n",
    "                    error=0.\n",
    "                    for unit in self._layers[i+1]:\n",
    "                        error+=unit['weights'][j]*unit['delta']\n",
    "                    layer[j]['delta']=error*self.derivate_b(layer[j]['output'])\n",
    "    \n",
    "    def update_weights(self,row):\n",
    "        for i in range(len(self._layers)):\n",
    "            layer=self._layers[i]\n",
    "            inputs=row\n",
    "            #for other layer except the first input is output of previous node\n",
    "            if(i!=0):\n",
    "                inputs=[unit['output'] for unit in self._layers[i-1]]\n",
    "            for unit in self._layers[i]:\n",
    "                unit['weights']+=unit['delta']*self._lr*np.array(inputs)\n",
    "            \n",
    "    def loss(self,X,Y):\n",
    "        #cross entropy loss\n",
    "        m=X.shape[0]\n",
    "        error=0.\n",
    "        for i in range(X.shape[0]):\n",
    "            output=self.forward_prop(X[i])\n",
    "            for j in range(Y.shape[1]):\n",
    "                error+=Y[i][j]*np.log(output[j])+(1-Y[i][j])*np.log(1-output[j])\n",
    "        return -error/m\n",
    "    \n",
    "    def train(self,X,Y):\n",
    "        X,Y=self.preprocessing(X,Y)\n",
    "        for step in range(self._max_steps):\n",
    "            for i in range(X.shape[0]):\n",
    "                self.forward_prop(X[i])\n",
    "                self.back_prop_error(Y[i])\n",
    "                self.update_weights(X[i])\n",
    "            print('Epoch:',step+1,'Loss:',self.loss(X,Y))\n",
    "                \n",
    "                    \n",
    "    def predict(self,X):\n",
    "        X,_=self.preprocessing(X,None)\n",
    "        res_full=[]\n",
    "        for i in range(X.shape[0]):\n",
    "            res=[]\n",
    "            self.forward_prop(X[i])\n",
    "            for unit in self._layers[-1]:\n",
    "                res.append(unit['output'])\n",
    "        return np.array(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 0, 1: 1}\n",
      "Epoch: 1 Loss: 2.09435501247\n",
      "Epoch: 2 Loss: 2.1218851488\n",
      "Epoch: 3 Loss: 1.73686615714\n",
      "Epoch: 4 Loss: 1.71462036615\n",
      "Epoch: 5 Loss: 1.64886829235\n",
      "Epoch: 6 Loss: 1.43478730448\n",
      "Epoch: 7 Loss: 1.16575271419\n",
      "Epoch: 8 Loss: 1.14283511784\n",
      "Epoch: 9 Loss: 1.28348088575\n",
      "Epoch: 10 Loss: 1.35504311472\n",
      "Epoch: 11 Loss: 1.35714150609\n",
      "Epoch: 12 Loss: 1.33503243455\n",
      "Epoch: 13 Loss: 1.31408547578\n",
      "Epoch: 14 Loss: 1.32168322558\n",
      "Epoch: 15 Loss: 1.36003662805\n",
      "Epoch: 16 Loss: 1.15798240137\n",
      "Epoch: 17 Loss: 0.202868477727\n",
      "Epoch: 18 Loss: 1.18211787208\n",
      "Epoch: 19 Loss: 0.107179227845\n",
      "Epoch: 20 Loss: 0.392346338373\n",
      "Epoch: 21 Loss: 0.0864094660002\n",
      "Epoch: 22 Loss: 0.214507412477\n",
      "Epoch: 23 Loss: 1.18262853987\n",
      "Epoch: 24 Loss: 0.0785932551699\n",
      "Epoch: 25 Loss: 0.0801239443937\n",
      "Epoch: 26 Loss: 0.0671650867795\n",
      "Epoch: 27 Loss: 0.0575599271884\n",
      "Epoch: 28 Loss: 0.0504555868529\n",
      "Epoch: 29 Loss: 0.045088951542\n",
      "Epoch: 30 Loss: 0.0408920208296\n",
      "Epoch: 31 Loss: 0.0374984035529\n",
      "Epoch: 32 Loss: 0.0346786489771\n",
      "Epoch: 33 Loss: 0.0322863632825\n",
      "Epoch: 34 Loss: 0.0302239049053\n",
      "Epoch: 35 Loss: 0.0284229551475\n",
      "Epoch: 36 Loss: 0.0268337458536\n",
      "Epoch: 37 Loss: 0.0254188800349\n",
      "Epoch: 38 Loss: 0.0241495831623\n",
      "Epoch: 39 Loss: 0.0230032967466\n",
      "Epoch: 40 Loss: 0.0219620588454\n",
      "Epoch: 41 Loss: 0.0210113741321\n",
      "Epoch: 42 Loss: 0.0201394040819\n",
      "Epoch: 43 Loss: 0.0193363746067\n",
      "Epoch: 44 Loss: 0.0185941356219\n",
      "Epoch: 45 Loss: 0.0179058290345\n",
      "Epoch: 46 Loss: 0.0172656353946\n",
      "Epoch: 47 Loss: 0.0166685784074\n",
      "Epoch: 48 Loss: 0.0161103725186\n",
      "Epoch: 49 Loss: 0.0155873029128\n",
      "Epoch: 50 Loss: 0.0150961301516\n",
      "Epoch: 51 Loss: 0.0146340137246\n",
      "Epoch: 52 Loss: 0.0141984502457\n",
      "Epoch: 53 Loss: 0.0137872230937\n",
      "Epoch: 54 Loss: 0.0133983610679\n",
      "Epoch: 55 Loss: 0.013030104204\n",
      "Epoch: 56 Loss: 0.0126808753188\n",
      "Epoch: 57 Loss: 0.0123492561737\n",
      "Epoch: 58 Loss: 0.0120339673869\n",
      "Epoch: 59 Loss: 0.0117338514101\n",
      "Epoch: 60 Loss: 0.0114478580233\n",
      "Epoch: 61 Loss: 0.0111750319149\n",
      "Epoch: 62 Loss: 0.0109145019958\n",
      "Epoch: 63 Loss: 0.0106654721645\n",
      "Epoch: 64 Loss: 0.010427213293\n",
      "Epoch: 65 Loss: 0.0101990562435\n",
      "Epoch: 66 Loss: 0.00998038576135\n",
      "Epoch: 67 Loss: 0.00977063511543\n",
      "Epoch: 68 Loss: 0.00956928137774\n",
      "Epoch: 69 Loss: 0.0093758412539\n",
      "Epoch: 70 Loss: 0.00918986738872\n",
      "Epoch: 71 Loss: 0.00901094508345\n",
      "Epoch: 72 Loss: 0.00883868937121\n",
      "Epoch: 73 Loss: 0.0086727424047\n",
      "Epoch: 74 Loss: 0.00851277111729\n",
      "Epoch: 75 Loss: 0.0083584651241\n",
      "Epoch: 76 Loss: 0.00820953483428\n",
      "Epoch: 77 Loss: 0.00806570974979\n",
      "Epoch: 78 Loss: 0.00792673692913\n",
      "Epoch: 79 Loss: 0.00779237959747\n",
      "Epoch: 80 Loss: 0.00766241588702\n",
      "Epoch: 81 Loss: 0.00753663769332\n",
      "Epoch: 82 Loss: 0.00741484963517\n",
      "Epoch: 83 Loss: 0.00729686810739\n",
      "Epoch: 84 Loss: 0.00718252041666\n",
      "Epoch: 85 Loss: 0.00707164399208\n",
      "Epoch: 86 Loss: 0.00696408566305\n",
      "Epoch: 87 Loss: 0.00685970099774\n",
      "Epoch: 88 Loss: 0.00675835369633\n",
      "Epoch: 89 Loss: 0.0066599150338\n",
      "Epoch: 90 Loss: 0.00656426334761\n",
      "Epoch: 91 Loss: 0.00647128356605\n",
      "Epoch: 92 Loss: 0.00638086677364\n",
      "Epoch: 93 Loss: 0.00629290981017\n",
      "Epoch: 94 Loss: 0.00620731490036\n",
      "Epoch: 95 Loss: 0.0061239893116\n",
      "Epoch: 96 Loss: 0.00604284503707\n",
      "Epoch: 97 Loss: 0.0059637985024\n",
      "Epoch: 98 Loss: 0.00588677029354\n",
      "Epoch: 99 Loss: 0.00581168490438\n",
      "Epoch: 100 Loss: 0.00573847050221\n"
     ]
    }
   ],
   "source": [
    "X=data[['x','y']].as_matrix()\n",
    "Y=data[['label']].as_matrix()\n",
    "\n",
    "nn=neural_net(2,2,2,lr=0.1,max_steps=100)\n",
    "nn.train(X,Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (3,) and (2,3) not aligned: 3 (dim 0) != 2 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-125-7906cac58a2c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-116-8b728a63127a>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_prop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m         \u001b[0mres\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0munit\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_layers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-116-8b728a63127a>\u001b[0m in \u001b[0;36mforward_prop\u001b[0;34m(self, row)\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0mnew_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0munit\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m                 \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0munit\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'weights'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m                 \u001b[0munit\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'output'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m                 \u001b[0mnew_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munit\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'output'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-116-8b728a63127a>\u001b[0m in \u001b[0;36moutput\u001b[0;34m(self, inputs, weights)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward_prop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (3,) and (2,3) not aligned: 3 (dim 0) != 2 (dim 0)"
     ]
    }
   ],
   "source": [
    "nn.predict(X[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
